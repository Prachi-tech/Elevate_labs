# Elevate_labs
Task  given by ElevateLabs in AI ML Internship

# Task 1: Data Cleaning & Preprocessing

### 🎯 Objective:
To clean and prepare raw data for Machine Learning using Python libraries such as Pandas, NumPy, Matplotlib, and Seaborn.

---

## 🛠️ Tools & Libraries Used:
- Python
- Pandas
- NumPy
- Seaborn
- Matplotlib
- Scikit-learn (for StandardScaler)

## 📁 Files Included:
- `task1_data_cleaning.ipynb` – Main Jupyter notebook with all code and outputs.
- `README.md` – This file.
- `data/titanic.csv` – Raw dataset.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# Task 2: Exploratory Data Analysis (EDA)

🎯 **Objective**  
To explore and visualize the dataset using Python to gain insights and understand feature relationships.

🛠️ **Tools & Libraries Used**
- Python
- Pandas
- NumPy
- Seaborn
- Matplotlib

📊 **Key Steps Performed**
- Analyzed distributions of features (Age, Fare, etc.)
- Explored relationships between features (e.g., Survival vs Sex/Class)
- Plotted heatmaps, histograms, box plots, and count plots
- Identified correlations and patterns in the dataset

📁 **Files Included**
- `Task2ML_internship.ipynb` – Jupyter notebook with all EDA visualizations
- `Task2_readme.md` – This file
- `Titanic-Dataset.csv` – Raw dataset used for analysis

✅ **Output**  
Visual insights and statistical summaries to guide feature selection and model building.


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


# Task 3: Supervised Machine Learning – Classification

🎯 **Objective**  
To build and evaluate classification models to predict survival on the Titanic dataset.

🛠️ **Tools & Libraries Used**
- Python
- Pandas
- Scikit-learn
- Matplotlib
- Seaborn

📊 **Key Steps Performed**
- Data cleaning and feature engineering
- Split dataset into training and testing sets
- Trained multiple classifiers (e.g., Logistic Regression, Decision Tree)
- Evaluated performance using confusion matrix, accuracy, precision, recall, and F1-score

📁 **Files Included**
- `Task_3_ML_intership.ipynb` – Jupyter notebook with model building and evaluation
- `Task_3_readme.md` / `Task_3_ML_readme.md` – This file
- `Titanic-Dataset.csv` – Dataset used for training and testing

✅ **Output**  
Trained classification models with performance metrics and visual evaluation.


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


# Task 4: Binary Classification with Logistic Regression

🎯 **Objective**  
To implement a binary classifier using Logistic Regression on the Breast Cancer Wisconsin dataset.

🛠️ **Tools & Libraries Used**
- Python
- Pandas
- Scikit-learn
- Matplotlib
- StandardScaler (from Scikit-learn)

📊 **Key Steps Performed**
- Loaded and cleaned dataset
- Encoded labels and dropped unnecessary columns
- Scaled features using StandardScaler
- Trained logistic regression model
- Evaluated using confusion matrix, precision, recall, F1-score, and ROC-AUC
- Tuned classification threshold to improve recall

📁 **Files Included**
- `Task_4_ML_.ipynb` – Jupyter notebook with all code and model evaluation
- `Task_4_readme.md` – This file
- `data.csv` – Breast Cancer dataset used for training and testing

✅ **Output**  
Accurate logistic regression model with ~96% accuracy and ROC-AUC of 1.00, ready for deployment or further analysis.

--------------------------------------------------------------------------------------

Task 5: Decision Trees and Random Forests

🎯 Objective
To apply tree-based models (Decision Tree and Random Forest) for classification using the Heart Disease dataset.

🛠️ Tools & Libraries Used

Python

Pandas
Scikit-learn
Graphviz
Matplotlib
Seaborn

📊 Key Steps Performed

Loaded and explored the heart.csv dataset
Trained a Decision Tree Classifier and visualized the tree
Controlled tree depth to prevent overfitting
Trained a Random Forest Classifier and compared performance
Interpreted feature importances and plotted using Seaborn
Evaluated model using 5-fold cross-validation

📁 Files Included

task5_ML_internship.ipynb – Jupyter notebook with all code and visualizations
heart.csv – Dataset used for training and testing
README.md – This file

✅ Output

Pruned Decision Tree Accuracy: 0.80
Random Forest Accuracy: 0.985
Cross-validation Mean Accuracy: ~99.7%
Random Forest model performed with excellent accuracy and generalization
Feature importance helped identify key predictors of heart disease

## 🔗 Author:
[Prachi] – B.Tech (IT) | ML Learner |
